{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every word we are going to have a 8 X 1 vector. There are 4 words so we are going to have 4 * 8 X 1 vectors. \n",
    "So, for query vector we have 4 word representations each of shape 8 X 1. Same for Key and Value vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "length, dk,dv = 4,8,8\n",
    "#length is the length of the inpute sentence or sequence\n",
    "#dk - size of the 4 vectors/words of input sentence \n",
    "#dv - for each word there are two vectors of size 8 \n",
    "\n",
    "q = np.random.randn(length,dk)\n",
    "k = np.random.randn(length,dk)\n",
    "v = np.random.randn(length,dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vector initialized to :  [[-1.05226786 -1.45622728 -1.49409858 -0.22401677  1.27378093  0.37489617\n",
      "   1.39358028 -2.04254455]\n",
      " [ 1.12208672  0.29292798  0.41313787 -2.65463817  0.1442023  -0.81215815\n",
      "   1.6343774   1.63381934]\n",
      " [-0.3734988  -1.20677705  0.17089084  0.7537739  -1.55316242  0.0325164\n",
      "  -0.20545718 -0.59549639]\n",
      " [-0.51613113 -0.18086092 -0.73833255  0.34159166 -0.02267987  0.3804978\n",
      "   0.03070229  0.31732767]]\n",
      "Key vector initialized to :  [[ 0.08967536 -0.73662656 -0.21155483 -0.34988207 -1.40330671  0.46893129\n",
      "  -0.31229733 -0.54981254]\n",
      " [ 0.61002556  0.52360371  0.16391828  0.22941029 -0.63037566 -0.65122393\n",
      "  -0.661597    0.03848236]\n",
      " [ 0.24159718  0.80764755  0.52046895 -1.26594196  0.91288491 -0.43786117\n",
      "  -1.37108249 -0.98343145]\n",
      " [-1.7183082  -1.39770631 -1.61196873 -0.38908481  0.28559876 -0.25596778\n",
      "  -1.26473164 -0.25460589]]\n",
      "Value vector initialized to :  [[-0.0535808  -0.22407469 -1.00780423  1.45763766 -0.22523626 -1.17556314\n",
      "  -0.51913612 -0.48569348]\n",
      " [ 0.11884949 -0.3599684  -0.05027791  1.33930202 -0.02792768  0.23451994\n",
      "  -0.65878092  0.4552944 ]\n",
      " [ 1.45131318  0.09776415 -0.1779497   2.06657147  0.16349351  2.4064706\n",
      "   0.73919497 -0.7301556 ]\n",
      " [ 1.24668578 -0.75215788  1.56849929 -0.46680514  1.12077006  0.14172575\n",
      "   0.5539424  -2.53392695]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query vector initialized to : \",q)\n",
    "print(\"Key vector initialized to : \",k)\n",
    "print(\"Value vector initialized to : \",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an initial attention matrix we need every word to look at every single other word to check its affinity toward the word. The word that we are looking for is the query. Word that we currently have is the key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44889684, -3.74839043, -0.82773109,  5.36446798],\n",
       "       [-1.2656585 , -0.28383146,  0.72295591, -4.20456257],\n",
       "       [ 3.14195194,  0.41213294, -2.494945  ,  1.71931393],\n",
       "       [ 0.14981889, -0.69380765, -1.62895679,  1.97342748]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q,k.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix multiplication results in a 4X4 matrix. Diving deep into it, the first word is going to focus the most on the 4th word. We want to minimize the variance of this matrix and stabilize the values. So we divide it by the square root of the dimension of the key vector or query vector ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0455276269036424"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5324130632969337"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.611252748084973"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q,k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0455276269036424, 0.5324130632969337, 0.7014065935106215)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q,k.T) / math.sqrt(dk)\n",
    "q.var(),k.var(),scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking is done ensure words do not get context from the words generated in the next or succeeding timesteps.\n",
    "It is not necessary in encoders but decoders need it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones( (length,length) ))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask==0] = -np.infty\n",
    "mask[mask==1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.158709  ,        -inf,        -inf,        -inf],\n",
       "       [-0.44747785, -0.10034957,        -inf,        -inf],\n",
       "       [ 1.11084776,  0.145711  , -0.88209626,        -inf],\n",
       "       [ 0.05296898, -0.24529805, -0.5759232 ,  0.69771198]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled+mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled+mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.41407898, 0.58592102, 0.        , 0.        ],\n",
       "       [0.65909816, 0.25107099, 0.08983085, 0.        ],\n",
       "       [0.23918967, 0.17750341, 0.12753166, 0.45577526]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention matrix consists of attention weights of the words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative infinity initialization resulted in zeroing by the softmax function. Here, in the attention matrix, the words only focus on the words with non zero attention weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying the attention matrix with the value matrix we get a better encapsulation of the contextual information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0535808 , -0.22407469, -1.00780423,  1.45763766, -0.22523626,\n",
       "        -1.17556314, -0.51913612, -0.48569348],\n",
       "       [ 0.04744973, -0.30369767, -0.44676943,  1.38830232, -0.10962902,\n",
       "        -0.34936582, -0.60095694,  0.0656511 ],\n",
       "       [ 0.12489735, -0.2292826 , -0.69285061,  1.48262806, -0.14077788,\n",
       "        -0.49975505, -0.44115993, -0.27139896],\n",
       "       [ 0.76157713, -0.44783889,  0.44220809,  0.63717761,  0.47283842,\n",
       "         0.13194181,  0.10563615, -1.28337573]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed_v = np.matmul(attention,v)\n",
    "computed_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q,k,v,mask=None):\n",
    "    dk = q.shape[-1]\n",
    "    scaled = np.matmul(q,k.T)/math.sqrt(dk)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask \n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention,v)\n",
    "    return out,attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
